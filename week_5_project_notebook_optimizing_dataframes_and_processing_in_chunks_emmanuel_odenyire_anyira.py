# -*- coding: utf-8 -*-
"""Week 5 Project Notebook: Optimizing DataFrames and Processing in Chunks - Emmanuel Odenyire Anyira

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CpcWIz3mFVBQGcWrQ961ILVPtcp0Yhc1

# Project Notebook: Optimizing DataFrames and Processing in Chunks

## 1. Introduction

In this project, we'll practice working with chunked dataframes and optimizing a dataframe's memory usage. We'll be working with financial lending data from Lending Club, a marketplace for personal loans that matches borrowers with investors. You can read more about the marketplace on its website.

The Lending Club's website lists approved loans. Qualified investors can view the borrower's credit score, the purpose of the loan, and other details in the loan applications. Once a lender is ready to back a loan, it selects the amount of money it wants to fund. When the loan amount the borrower requested is fully funded, the borrower receives the money, minus the origination fee that Lending Club charges.

We'll be working with a dataset of loans approved from 2007-2011 (https://bit.ly/3H2XVgC). We've already removed the desc column for you to make our system run more quickly.

If we read in the entire data set, it will consume about 67 megabytes of memory. Let's imagine that we only have 10 megabytes of memory available throughout this project, so you can practice the concepts you learned in the last two lessons.

**Tasks**

1. Read in the first five lines from `loans_2007.csv` (https://bit.ly/3H2XVgC) and look for any data quality issues.

2. Read in the first 1000 rows from the data set, and calculate the total memory usage for these rows. Increase or decrease the number of rows to converge on a memory usage under five megabytes (to stay on the conservative side).

*We import the Pandas library and set the maximum number of columns to be displayed to 99. We then read a csv file from the URL 'https://bit.ly/3H2XVgC' using the read_csv method from the Pandas library, and display the first 5 rows of the resulting data frame using the head() method.*
"""

# Importing pandas
import pandas as pd
pd.options.display.max_columns = 99

#Reading data
approved_loans=pd.read_csv('https://bit.ly/3H2XVgC')
approved_loans.head()

"""*We read the csv file from 'https://bit.ly/3H2XVgC' using the read_csv method from the Pandas library, but only reads the first 1000 rows by specifying nrows = 1000. We then calculate the memory usage of the resulting data frame in megabytes using the memory_usage method and the sum method, and display the result. The calculation is done by summing the memory usage of each column in the data frame and dividing the result by 1024**2 to convert it to megabytes.*"""

df_approved_loans= pd.read_csv("https://bit.ly/3H2XVgC", nrows = 1000)
print("usage(MB) for 1000 rows: ",df_approved_loans.memory_usage(deep=True).sum()/(1024**2))

"""## 2. Exploring the Data in Chunks

Let's familiarize ourselves with the columns to see which ones we can optimize. In the first lesson, we explored column types by reading in the full dataframe. In this project, let's try to understand the column types better while using dataframe chunks.

**Tasks**

For each chunk:
* How many columns have a numeric type? 
* How many have a string type?
* How many unique values are there in each string column? How many of the string columns contain values that are less than 50% unique?
* Which float columns have no missing values and could be candidates for conversion to the integer type?
* Calculate the total memory usage across all of the chunks.

*We read the CSV file from the given URL in chunks of size 3000 rows. We then create a list called "chunk_types" which stores the data types of each chunk as a pandas DataFrame*
"""

# Your code goes here
df_chunk=pd.read_csv('https://bit.ly/3H2XVgC', chunksize=3000)
chunk_types=[c.dtypes for c in df_chunk]

"""Task 1: How many columns have a numeric type?

*We perform a value count on the first element of a list named "chunk_types to check the data types."*
"""

chunk_types[0].value_counts()

"""Task 2: How many have a string type?

The code "chunk_types[0].value_counts()" shows all the data types

Task 3: How many unique values are there in each string column?

*We identify the columns in the dataframe represented by the first element of the "chunk_types" list that have a data type of 'object', and store the names of these columns in the "lsObjCols" list.*
"""

colObj = chunk_types[0].loc[lambda x : x == 'object']
lsObjCols = colObj.index.values.tolist()
lsObjCols

"""Task 3 Part 2: How many of the string columns contain values that are less than 50% unique?

*We read the csv file in chunks and process each chunk by converting a column to numeric values, dropping missing values, selecting columns with data type "object," calculating the percentage of unique values, concatenating the results, grouping and calculating the mean, and finally identifying columns with a mean percentage of unique values less than 50.*
"""

from enum import unique

approved_loan_df = pd.read_csv('https://bit.ly/3H2XVgC', chunksize = 2000)

percentage = []
for chunk in approved_loan_df:
  chunk["id"] = pd.to_numeric(chunk["id"], errors = "coerce")
  chunk = chunk.dropna(axis = 0, subset = ["id"])
  unique_string_columns = chunk.select_dtypes(include=["object"]).nunique()
  count_string_columns = chunk.select_dtypes(include=["object"]).count()
  percentage.append((100*unique_string_columns/count_string_columns))
  unique_total = pd.concat(percentage)
  unique_total = unique_total.groupby(unique_total.index).mean()
  category_string_columns = list((unique_total[unique_total<50]).index)

print(print(f"\npercentage of unique values is: \n {unique_total}"))

"""Task 4: Which float columns have no missing values and could be candidates for conversion to the integer type?

* We identify the columns in the dataframe represented by the first element of the "chunk_types" list that have a data type of 'float64', and store the names of these columns in the "lsFloatCols" list.*

Code 1: This first code doesn't check if we have missing values. This is because we had already dropped the missing values
"""

colFloat = chunk_types[0].loc[lambda x : x == 'float64']
lsFloatCols = colFloat.index.values.tolist()
lsFloatCols

"""Code 2: This code snippet does both

> In this modification, you first select the float64 columns in the same way as in the original code. Then, you use the chunk[colFloat.index] syntax to select only the float64 columns from the chunk dataframe. Next, you use the dropna method to remove any rows that contain missing values, using the how argument set to 'any' to remove rows with any missing values. Finally, you retrieve the list of column names of the resulting dataframe using the columns attribute and store it in the lsFloatCols list.


"""

colFloat = chunk_types[0].loc[lambda x : x == 'float64']
float_cols = chunk[colFloat.index].dropna(how='any', axis=0)
lsFloatCols = float_cols.columns.tolist()
lsFloatCols

"""Task 5: Calculate the total memory usage across all of the chunks.

*We load the csv file from the given URL in chunks of 2000 rows at a time. We the calculate the memory usage of each chunk, appends it to a list called initial_memory, and then calculates the total memory usage by summing up the elements in the initial_memory list. Finally, it prints a message that gives the total memory usage in MB.*
"""

approved_loan_df = pd.read_csv('https://bit.ly/3H2XVgC', chunksize = 2000)
initial_memory = []
for chunk in approved_loan_df:
  initial_memory.append(chunk.memory_usage(deep=True).sum()/(1024**2))

print("total memory usage: {:.4f} MB" . format(sum(initial_memory)))

"""## 3. Optimizing String Columns

We can achieve the greatest memory improvements by converting the string columns to a numeric type. Let's convert all of the columns where the values are less than 50% unique to the category type, and the columns that contain numeric values to the `float` type.

While working with dataframe chunks:
* Determine which string columns you can convert to a numeric type if you clean them. For example, the `int_rate` column is only a string because of the % sign at the end.
* Determine which columns have a few unique values and convert them to the category type. For example, you may want to convert the grade and `sub_grade` columns.
Based on your conclusions, perform the necessary type changes across all chunks. * Calculate the total memory footprint, and compare it with the previous one.

Task 1:- Determine which string columns you can convert to a numeric type if you clean them. For example, the int_rate column is only a string because of the % sign at the end.

*We create a dictionary called convert_columns_dtypes, which maps four columns in the dataframe to new data types.We use these values to convert the string data types to numeric data types using the astype method, thus saving memory compared to the object data type.*
"""

# Your code goes here
convert_columns_dtypes = {"sub_grade":"category", "home_ownership":"category", "verification_status":"category", "purpose":"category"}

"""Task 2:- Determine which columns have a few unique values and convert them to the category type. For example, you may want to convert the grade and sub_grade columns. Based on your conclusions, perform the necessary type changes across all chunks. * Calculate the total memory footprint, and compare it with the previous one.

*We read a CSV file into a Pandas dataframe in chunks of size 2000 and specifies the data types for some columns using the dtype argument. We then perform some data cleaning and transformation, such as stripping whitespace from certain columns, converting percentage symbols to numeric type, and converting certain columns to datetime data type using the parse_dates argument. Finally, we then calculate the memory usage of each chunk and displays the total memory usage of the entire dataframe, which has been reduced through data cleaning and data type optimization.*

>*Note that, we have done stripping here in this code to allow code readability, although this was expected in Task 1.*
"""

# Your code goes here
approved_loan_df = pd.read_csv('https://bit.ly/3H2XVgC', chunksize = 2000, dtype = convert_columns_dtypes, parse_dates = ["issue_d", "earliest_cr_line", "last_pymnt_d", "last_credit_pull_d"])
total_memory=[]
for chunk in approved_loan_df:
  term_cleaned=chunk["term"].str.lstrip(" ").str.rstrip("months")
  int_rate_cleaned=chunk["int_rate"].str.rstrip("%")
  revol_cleaned=chunk["revol_util"].str.rstrip("%")
  chunk["term"]=pd.to_numeric(term_cleaned)
  chunk["revol_util"]=pd.to_numeric(revol_cleaned)
  chunk["int_rate"]=pd.to_numeric(int_rate_cleaned)
  total_memory.append(chunk.memory_usage(deep=True).sum()/(1024**2))

print("\nTotal memory usage with string optimisation: {:.2f} MB" . format(sum(total_memory)))

"""## 4. Optimizing Numeric Columns

It looks like we were able to realize some powerful memory savings by converting to the category type and converting string columns to numeric ones.

Now let's optimize the numeric columns using the `pandas.to_numeric()` function.

**Tasks**

While working with dataframe chunks:
* Identify float columns that contain missing values, and that we can convert to a more space efficient subtype.
* Identify float columns that don't contain any missing values, and that we can convert to the integer type because they represent whole numbers.
* Based on your conclusions, perform the necessary type changes across all chunks.
* Calculate the total memory footprint and compare it with the previous one.

*A pandas dataframe is read in chunks with specified data types and dates. We then perform several data cleaning operations such as removing leading and trailing whitespaces and converting strings to numeric types. For float columns, we check if the column has missing values or not. If the column has missing values, we downcast the float to reduce memory usage, and if the column doesn't have missing values, we cast the float to an integer type if the values in the column are within the range of the integer type. Finally, we calculate the total memory usage of the dataframe after these operations and the percentage of memory savings achieved.*
"""

# Your code 
import numpy as np
def change_to_int(approved_loan_df, column_name):
  max_column=approved_loan_df[column_name].max()
  min_column=approved_loan_df[column_name].min()
  for dtype_name in ["int8", "int16", "int32", "int64"]:
    if max_column < np.iinfo(dtype_name).max and min_column > np.iinfo(dtype_name).min:
      df[column_name] = df[column_name].astype(dtype_name)
      break

total_memory = []
for chunk in approved_loan_df:
  chunk["id"] = pd.to_numeric(chunk["id"], errors = "coerce")
  chunk = chunk.dropna(axis=0, subset = ["id"])
  term_cleaned=chunk["term"].str.lstrip(" ").str.rstrip(" months")
  int_rate_cleaned=chunk["int_rate"].str.rstrip("%")
  revol_cleaned=chunk["revol_util"].str.rstrip("%")
  chunk["term"]=pd.to_numeric(term_cleaned)
  chunk["revol_util"]=pd.to_numeric(revol_cleaned)
  chunk["int_rate"]=pd.to_numeric(int_rate_cleaned)
  float_columns=chunk.select_dtypes(include=["float"])
  float_columns=float_columns.dropna()
  for columns in float_columns.columns:
    if columns in missing:
      chunk[columns]=pd.to_numeric(chunk[columns], downcast="float")
    elif columns in no_missing:
      change_to_int(chunk, columns)
  total_memory.append(chunk.memory_usage(deep=True).sum()/(1024**2))

print("\nTotal memory usage: {:.2f} MB" . format(sum(total_memory)))
print("\nPercentage memory savings: {:.2f} %" .format(100*(sum(initial_memory) - sum(total_memory))/sum(initial_memory)))
print(f"\n {chunk.dtypes}")

"""## Next Steps

We've practiced optimizing a dataframe's memory footprint and working with dataframe chunks. Here's an idea for some next steps:

Create a function that automates as much of the work you just did as possible, so that you could use it on other Lending Club data sets. This function should:

* Determine the optimal chunk size based on the memory constraints you provide.

* Determine which string columns can be converted to numeric ones by removing the `%` character.

* Determine which numeric columns can be converted to more space efficient representations.

Task 1:- Determine the optimal chunk size based on the memory constraints you provide.

*The optimal_chunk_size function determines the optimal chunk size for reading a CSV file into a Pandas dataframe while limiting the memory usage to a desired value. We use a while loop to increment the number of rows in each chunk and keep track of the memory usage until it exceeds the desired value. The chunk size is then reduced and returned, allowing the data to be efficiently loaded into memory in multiple smaller chunks.*
"""

# Your code here

def optimal_chunk_size(csv_file, desired_mem, row_steps, start_chunk_size):
  no_rows = start_chunk_size
  chunk_memory =0
  while(chunk_memory < desired_mem):
    no_rows += row_steps
    chunk = pd.read_csv(csv_file, nrows=no_rows)
    chunk_memory = chunk.memory_usage(deep=True).sum()/(1024**2)
  return(no_rows - row_steps) # reduce by 1

optimal_chunk_size('https://bit.ly/3H2XVgC', 5, 50, 2500)

"""Task 2:- Determine which string columns can be converted to numeric ones by removing the % character

*We define a function string_to_numeric that takes a pandas DataFrame and a character as input. The function identifies columns in the DataFrame that contain the specified character and converts those columns to numeric format. The names of the columns that are successfully converted are stored in a list and printed at the end of the function. We then load a DataFrame loans from a CSV file, selects only the string columns, and applies the function to those columns, passing the percent symbol % as the specified character.*
"""

# Your code here
def string_to_numeric(df, character):
  columns_with_char = []
  for key, value in df.iteritems():
    if  value.str.contains(character).any(): #check if column has % character
      try:
        value = value.str.replace('%', '')  #remove it and try to convert to numeric
        value = pd.to_numeric(value, errors='raise') 
        columns_with_char.append(key)
      except(ValueError):
        #dont add columns to the list
        continue
  print(columns_with_char)

loans = pd.read_csv('https://bit.ly/3H2XVgC')
string_cols = loans.select_dtypes(include='object')
string_to_numeric(string_cols, '%')

"""Task 3:- Determine which numeric columns can be converted to more space efficient representations

*We define a function numeric_mem_optimise that takes a pandas DataFrame as input. The function optimizes the memory usage of the DataFrame by converting numeric columns with missing values to floating point type and numeric columns without missing values to integer type. We then return two lists of column names, one for integer type columns and one for floating point type columns. The code then loads a DataFrame from a CSV file, converts one column to numeric type, removes any rows with missing values in that column, and calls the function on the DataFrame. The two lists of column names are then printed.*
"""

# Your code here

def numeric_mem_optimise(df): 
  
  numeric_cols = df.select_dtypes(exclude=['object'])
  cols_missing_value = numeric_cols.isnull().sum()
 
  int_cols = list(cols_missing_value[cols_missing_value == 0].index)
  float_cols = list(cols_missing_value[cols_missing_value >0].index)

  return int_cols, float_cols

loans = pd.read_csv('https://bit.ly/3H2XVgC')
loans['id']=pd.to_numeric(loans['id'], errors='coerce')
loans.dropna(axis=0, subset=['id'], inplace=True)

x,y = numeric_mem_optimise(loans)
print(f'int columns: {x}')
print(f'float columns: {y}')

# A different code which does the same thing

import pandas as pd

def optimize_df(df, max_mem_usage):
    # Step 1: Determine the optimal chunk size
    chunk_size = df.memory_usage().sum() / max_mem_usage
    chunk_size = int(chunk_size) + 1
    
    # Step 2: Convert string columns to numeric
    for col in df.columns:
        if df[col].dtype == "object":
            try:
                df[col] = pd.to_numeric(df[col].str.strip("%").astype(float))
            except:
                pass
                
    # Step 3: Convert numeric columns to more space efficient representation
    num_cols = df.select_dtypes(include=['float', 'int']).columns
    for col in num_cols:
        if ((df[col].max() <= 127) and (df[col].min() >= -128)):
            df[col] = df[col].astype('int8')
        elif ((df[col].max() <= 32767) and (df[col].min() >= -32768)):
            df[col] = df[col].astype('int16')
        elif ((df[col].max() <= 2147483647) and (df[col].min() >= -2147483648)):
            df[col] = df[col].astype('int32')
        else:
            df[col] = df[col].astype('float32')
    
    return df

"""*We can also try this code snippet.*"""

import pandas as pd

def optimizer(file, mem_limit, chunk_step):
    # Step 1: Determine the optimal chunk size
    chunks = pd.read_csv(file, chunksize=chunk_step)
    mem_usage = 0
    chunk_size = chunk_step
    for chunk in chunks:
        mem_usage = chunk.memory_usage().sum()
        if mem_usage > mem_limit:
            chunk_size = chunk_size // 2
            break
    chunk_size = max(chunk_size, 1)
    
    # Step 2: Read the data in chunks and optimize memory usage
    optimized_df = pd.DataFrame()
    chunks = pd.read_csv(file, chunksize=chunk_size)
    for chunk in chunks:
        # Convert string columns to numeric
        for col in chunk.columns:
            if chunk[col].dtype == "object":
                try:
                    chunk[col] = pd.to_numeric(chunk[col].str.strip("%").astype(float))
                except:
                    pass

        # Convert numeric columns to more space efficient representation
        num_cols = chunk.select_dtypes(include=['float', 'int']).columns
        for col in num_cols:
            if ((chunk[col].max() <= 127) and (chunk[col].min() >= -128)):
                chunk[col] = chunk[col].astype('int8')
            elif ((chunk[col].max() <= 32767) and (chunk[col].min() >= -32768)):
                chunk[col] = chunk[col].astype('int16')
            elif ((chunk[col].max() <= 2147483647) and (chunk[col].min() >= -2147483648)):
                chunk[col] = chunk[col].astype('int32')
            else:
                chunk[col] = chunk[col].astype('float32')
        
        optimized_df = pd.concat([optimized_df, chunk], ignore_index=True)
    
    return optimized_df